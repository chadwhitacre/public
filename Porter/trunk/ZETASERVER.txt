Zetaserver is a distributed http server architecture. We use mature tools as
much as possible, with only one custom program. Here's what you need:

    1) Apache 1.3

    2) Python 2.4

    3) BIND 8

    4) Porter 0.2

Here's our use case: we have multiple websites that are served by a variety of
application servers (Zope 2.6, Zope 2.7, php, svn, straight httpd). For
stability, security, and sanity, we want to run the app servers on different
machines. However, we want the public Internet to think that our websites are
served by a single httpd. Here's why:

    1) Instant routing -- All DNS points to our single master httpd, and then we
    use Porter to route requests to our backend servers based on the domain
    name. This vastly streamlines staging, rollout, and fallback, as we never
    have to wait for DNS propagation.

    2) Centralized caching

    3) Centralized logging

Note that this is the opposite of a load-balancing scenario, where you have a
single website that you want to distribute across the resources of multiple
machines. If you had a website in a Zetaserver that needed to be load-balanced,
there's no reason why you couldn't point Porter to a load balancer.

    request -> Porter -> load balancer -> application

Implementing a Zetaserver therefore has three steps:

    1) setup the master httpd

    2) setup your backend servers

    3) point all your DNS to the master httpd


NOTE: there are three error conditions:

    1) We don't know about the domain at all. This is handled by the first vhost
    and a "parked" page.

    2) The backend server is down. To handle this we have three options:

        a. Maintain a custom "proxy error" page on this server that is at least
        not as ugly. LIKELY BEST OPTION

        b. Make porter smart enough to remove or withhold domains with downed
        servers (would mean proactively monitoring the servers). Porter needs to
        be dumb, imo. If we start making it smart then we need to look into
        Pound.

        c. Somehow handle it on the server end, so that there is a fallback or
        something.

    3) The hostname of the proxy server is wrong in porter. So if we are trying
    to point example.com to notthere:8080 then we will get a Proxy Error,
    Reason: Host not found (after a timeout period). For this we should


issue: there needs to be a VirtualHostMonster in the Zope in order to use it
through porter. That means that we need to be able to hit a new Zope at least
once ttw in order to configure VHM. The problem with that is that we had hoped
to close off all ports except 80. Possible solutions:

    1) leave ports besides 80 open so we can hit them directly

    2) route all direct zope traffic through porter on 8080

    3) add a switching feature (to porter? to ipfw?) so that we can temporarily
    open up holes to zopes when we need to

    #1 and #2 have the same net effect but #2 would take more work. #3 is the
    real solution, so prolly #1 + #3.

    4) never create a new zope on Zetaserver!! only add zopes on zetaserver
    which we have created elsewhere (i.e., josemaria), and have then tar'd and
    are rolling out there. I LIKE THIS ONE because it emphasizes the
    "production" nature of zetaserver. We do use it for staging though (think
    edp today :-/ ), so maybe some combo.

    5) customize the zope skel on zope farm servers to include a Data.fs which
    is pre-configured with a VHM. We really don't want to stray far from stock
    Zope setup at all though.

The other issue with that is that we definitely want to be able to get to the
root of our Zopes ttw, which means that each zope needs a domain name. That's ok
though, it just means that when we start pulling a list of servers from
/etc/hosts that we need to stick a record in named.conf.frag for each one, i.e.,
bridei.zetaserver.com. Except that bridei represents many zopes (that's the
whole point). Right so what that means is that subdomains of zetaserver.com
should be proxied w/o VirtualHostMonster, i.e., should hit directly.

    example.com -> server:port/VHM/example.com/etc
    server_port.zetaserver.com -> server:port/etc
    server.zetaserver.com -> server:8000 # not sure we want to actually expose this publically

But here's the thing, all of our servers have public IP addresses, so even when
they are accessed through porter they are being proxied through the public
Internet. If we really wanted them to be private then we would have to do NAT.
Now, we definitely want to be able to ssh directly to all of our servers. What
is it that we want? We want the jails to be accessed directly only via ssh. We
want all http to go through porter. Why? Again, three reasons:

    1) instant routing

    2) caching

    3) logging

#3 is the only security-related consideration.

Now, I don't think firewalling is going to work, since that is NIC level and by
using mod_proxy (which is what is behind mod_rewrite's proxying) we are in fact
still connecting through TCP. However, if there aren't actually any domains
pointing to the backend IP addresses then you would have to know the IP to
connect to that machine.

Now, IF someone connected directly to a backend server by IP address, what would
they find? Assuming we've turned off everything we don't need (named, sendmail,
inetd), they would find, say, a number of zope servers running on various ports,
and ssh. Accessing these zope servers directly by ip:port is almost exactly like
accessing them through porter, with the exception that our central logging is
circumvented. Since our centralized logs will be the basis of our early warning
system, this poses a security risk, since we wouldn't actually know that someone
is up to something. I think really we want to log to a separate logging server
both from porter and from the "court."

also an issue: we need to rewrite requests differently for different
applications. I.e., zope wants VirtualHostMonster in there, but that doesn't
mean anything to straight Apache. ugh, that means we may need to make porter
know about different applications and maintain different rewrite.db's for each
app.

we don't actually handle DNS for tesm.edu -- does it matter if we still have a record for them?
